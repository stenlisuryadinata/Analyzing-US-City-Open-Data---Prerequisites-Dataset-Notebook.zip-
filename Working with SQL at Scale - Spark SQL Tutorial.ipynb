{"cells":[{"cell_type":"markdown","source":["## SQL at Scale with Spark SQL and DataFrames\n\nSpark SQL brings native support for SQL to Spark and streamlines the process of querying data stored both in RDDs (Sparkâ€™s distributed datasets) and in external sources. Spark SQL conveniently blurs the lines between RDDs and relational tables. Unifying these powerful abstractions makes it easy for developers to intermix SQL commands querying external data with complex analytics, all within in a single application. Concretely, Spark SQL will allow developers to:\n\n- Import relational data from Parquet files and Hive tables\n- Run SQL queries over imported data and existing RDDs\n- Easily write RDDs out to Hive tables or Parquet files\n\nSpark SQL also includes a cost-based optimizer, columnar storage, and code generation to make queries fast. At the same time, it scales to thousands of nodes and multi-hour queries using the Spark engine, which provides full mid-query fault tolerance, without having to worry about using a different engine for historical data.\n\n\nThis tutorial will familiarize you with essential Spark capabilities to deal with structured data typically often obtained from databases or flat files. We will explore typical ways of querying and aggregating relational data by leveraging concepts of DataFrames and SQL using Spark. We will work on an interesting dataset from the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) and try to query the data using high level abstrations like the dataframe which has already been a hit in popular data analysis tools like R and Python. We will also look at how easy it is to build data queries using the SQL language which you have learnt and retrieve insightful information from our data. This also happens at scale without us having to do a lot more since Spark distributes these data structures efficiently in the backend which makes our queries scalable and as efficient as possible."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5375778d-8c79-4a8d-b772-6027f2cecf39"}}},{"cell_type":"code","source":["import pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bcb92233-2396-46f1-b920-7caca25fd871"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Data Retrieval\n\nWe will use data from the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), which is the data set used for The Third International Knowledge Discovery and Data Mining Tools Competition, which was held in conjunction with KDD-99 The Fifth International Conference on Knowledge Discovery and Data Mining. The competition task was to build a network intrusion detector, a predictive model capable of distinguishing between \"bad\" connections, called intrusions or attacks, and \"good\" normal connections. This database contains a standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment. \n\nWe will be using the reduced dataset `kddcup.data_10_percent.gz` containing nearly half million nework interactions since we would be downloading this Gzip file from the web locally and then work on the same. If you have a good, stable internet connection, feel free to download and work with the full dataset available as `kddcup.data.gz`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf14f21f-8678-441c-9622-231e9bb7c1c7"}}},{"cell_type":"markdown","source":["## Building the KDD Dataset\n\nNow that we have our data stored in the Databricks filesystem. Let's load up our data from the disk into Spark's traditional abstracted data structure, the [Resilient Distributed Dataset (RDD)](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25fd8b32-0746-47c1-a915-b858c9f028e5"}}},{"cell_type":"code","source":["data_file = \"/FileStore/tables/kddcup_data_10_percent.gz\"\nraw_rdd = sc.textFile(data_file).cache()\nraw_rdd.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19b42046-ecca-447d-b6f7-6517b8ef3a1b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["type(raw_rdd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"946aed5b-1adb-4b47-81d4-4fa1cd86d9e2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Building a Spark DataFrame on our Data\n\nA Spark DataFrame is an interesting data structure representing a distributed collecion of data. A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a dataframe in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs in our case."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd411eec-1e13-40fd-9a09-a8358b77723e"}}},{"cell_type":"markdown","source":["#### Splitting the CSV data\nEach entry in our RDD is a comma-separated line of data which we first need to split before we can parse and build our dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43ad5980-5e33-480b-a6de-ab07e9357154"}}},{"cell_type":"code","source":["csv_rdd = raw_rdd.map(lambda row: row.split(\",\"))\nprint(csv_rdd.take(2))\nprint(type(csv_rdd))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1dd10e57-d21b-46a5-929d-d9f5c33c6cdb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Check the total number of features (columns)\nWe can use the following code to check the total number of potential columns in our dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c343b850-bc93-407b-a0f9-de2edfa89562"}}},{"cell_type":"code","source":["len(csv_rdd.take(1)[0])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"719977f5-959f-405b-8aab-43f8304e6988"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Data Understanding and Parsing\n\nThe KDD 99 Cup data consists of different attributes captured from connection data. The full list of attributes in the data can be obtained [__here__](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names) and further details pertaining to the description for each attribute\\column can be found [__here__](http://kdd.ics.uci.edu/databases/kddcup99/task.html). We will just be using some specific columns from the dataset, the details of which are specified below.\n\n\n| feature num | feature name       | description                                                  | type       |\n|-------------|--------------------|--------------------------------------------------------------|------------|\n| 1           | duration           | length (number of seconds) of the connection                 | continuous |\n| 2           | protocol_type      | type of the protocol, e.g. tcp, udp, etc.                    | discrete   |\n| 3           | service            | network service on the destination, e.g., http, telnet, etc. | discrete   |\n| 4           | src_bytes          | number of data bytes from source to destination              | continuous |\n| 5           | dst_bytes          | number of data bytes from destination to source              | continuous |\n| 6           | flag               | normal or error status of the connection                     | discrete   |\n| 7           | wrong_fragment     | number of ``wrong'' fragments                                | continuous |\n| 8           | urgent             | number of urgent packets                                     | continuous |\n| 9           | hot                | number of ``hot'' indicators                                 | continuous |\n| 10          | num_failed_logins  | number of failed login attempts                              | continuous |\n| 11          | num_compromised    | number of ``compromised'' conditions                         | continuous |\n| 12          | su_attempted       | 1 if ``su root'' command attempted; 0 otherwise              | discrete   |\n| 13          | num_root           | number of ``root'' accesses                                  | continuous |\n| 14          | num_file_creations | number of file creation operations                           | continuous |\n\nWe will be extracting the following columns based on their positions in each datapoint (row) and build a new RDD as follows"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a33f43d-9ebe-4e17-930d-27a21ade7ffb"}}},{"cell_type":"code","source":["from pyspark.sql import Row\n\nparsed_rdd = csv_rdd.map(lambda r: Row(\n    duration=int(r[0]), \n    protocol_type=r[1],\n    service=r[2],\n    flag=r[3],\n    src_bytes=int(r[4]),\n    dst_bytes=int(r[5]),\n    wrong_fragment=int(r[7]),\n    urgent=int(r[8]),\n    hot=int(r[9]),\n    num_failed_logins=int(r[10]),\n    num_compromised=int(r[12]),\n    su_attempted=r[14],\n    num_root=int(r[15]),\n    num_file_creations=int(r[16]),\n    label=r[-1]\n    )\n)\nparsed_rdd.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf42a6df-6634-4c06-887c-e8e0a94783ab"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Constructing the DataFrame\nNow that our data is neatly parsed and formatted, let's build our DataFrame!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cab8caed-98d7-48a0-8d5d-5b5fef1c6053"}}},{"cell_type":"code","source":["df = spark.createDataFrame(parsed_rdd)\ndisplay(df.head(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff2f6d32-07ba-4a37-80d2-40a2348e5d50"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now, we can easily have a look at our dataframe's schema using tne `printSchema(...)` function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f9e714b-d5ac-4fb7-89e5-0d9d2916a30f"}}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fd91bfc-1e6e-4b78-af5b-6be0082695b6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Building a temporary table \n\nWe can leverage the `createOrReplaceTempView()` function to build a temporaty table to run SQL commands on our DataFrame at scale! A point to remember is that the lifetime of this temp table is tied to the session. It creates an in-memory table that is scoped to the cluster in which it was created. The data is stored using Hive's highly-optimized, in-memory columnar format. \n\nYou can also check out `saveAsTable()` which creates a permanent, physical table stored in S3 using the Parquet format. This table is accessible to all clusters. The table metadata including the location of the file(s) is stored within the Hive metastore.`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd253e7f-c721-4385-b195-8ab6be423fdd"}}},{"cell_type":"code","source":["df.createOrReplaceTempView(\"connections\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e32a9c8-78c2-4fdf-927d-ab6db4eb127e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Executing SQL at Scale\nLet's look at a few examples of how we can run SQL queries on our table based off our dataframe. We will start with some simple queries and then look at aggregations, filters, sorting, subqueries and pivots"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e137bc84-380d-4734-8759-48954cc011a5"}}},{"cell_type":"markdown","source":["### Connections based on the protocol type\n\nLet's look at how we can get the total number of connections based on the type of connectivity protocol. First we will get this information using normal DataFrame DSL syntax to perform aggregations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fae562eb-ae7f-4749-801c-5361349f48dd"}}},{"cell_type":"code","source":["display(df.groupBy('protocol_type')\n          .count()\n          .orderBy('count', ascending=False))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c03bb30-7830-47ac-aacf-85a347ab7ae8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Can we also use SQL to perform the same aggregation? Yes we can leverage the table we built earlier for this!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8f27139-2405-41ba-a0c9-4bb07e10790a"}}},{"cell_type":"code","source":["protocols = spark.sql(\"\"\"\n                           SELECT protocol_type, count(*) as freq\n                           FROM connections\n                           GROUP BY protocol_type\n                           ORDER BY 2 DESC\n                           \"\"\")\ndisplay(protocols)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7dfa9ef1-482d-4aed-bfcf-f2d7fd38ff0a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can clearly see, that you get the same results and you do not need to worry about your background infrastructure or how the code is executed. Just write simple SQL!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2d81b4e-244c-4892-a95c-e5ad4332c89f"}}},{"cell_type":"markdown","source":["### Connections based on good or bad (attack types) signatures\n\nWe will now run a simple aggregation to check the total number of connections based on good (normal) or bad (intrusion attacks) types."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67cacb14-6512-4856-95e7-f96eb3b980f1"}}},{"cell_type":"code","source":["labels = spark.sql(\"\"\"\n                           SELECT label, count(*) as freq\n                           FROM connections\n                           GROUP BY label\n                           ORDER BY 2 DESC\n                           \"\"\")\ndisplay(labels)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0eab8b2-f14c-4bf0-abd8-802690715fe2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We have a lot of different attack types. We can visualize this in the form of a bar chart. The simplest way is to use the excellent interface options in the Databricks notebook itself as depicted in the following figure!\n\n![](https://cdn-images-1.medium.com/max/800/1*MWtgLR6H4siUB1Ugc8sqog.png)\n\nThis gives us the following nice looking bar chart! Which you can customize further by clicking on __`Plot Options`__ as needed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce5f90aa-1f02-436e-aa6e-8964e1ca98aa"}}},{"cell_type":"code","source":["labels = spark.sql(\"\"\"\n                           SELECT label, count(*) as freq\n                           FROM connections\n                           GROUP BY label\n                           ORDER BY 2 DESC\n                           \"\"\")\ndisplay(labels)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcda1e3e-9e27-47b1-9ffa-b35a551eac8c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Another way is to write the code yourself to do it. You can extract the aggregated data as a `pandas` DataFrame and then plot it as a regular bar chart."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53801bdf-d759-42d3-b386-7e9dbce0bb1c"}}},{"cell_type":"code","source":["labels_df = pd.DataFrame(labels.toPandas())\nlabels_df.set_index(\"label\", drop=True,inplace=True)\nlabels_fig = labels_df.plot(kind='barh')\n\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nplt.rcParams.update({'font.size': 10})\nplt.tight_layout()\ndisplay(labels_fig.figure)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78c00194-0edf-4378-9704-42aa16741aa8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Connections based on protocols and attacks\n\nLet's look at which protocols are most vulnerable to attacks now based on the following SQL query."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9153cd4-4d12-488b-aea5-2ea749774dea"}}},{"cell_type":"code","source":["attack_protocol = spark.sql(\"\"\"\n                           SELECT \n                             protocol_type, \n                             CASE label\n                               WHEN 'normal.' THEN 'no attack'\n                               ELSE 'attack'\n                             END AS state,\n                             COUNT(*) as freq\n                           FROM connections\n                           GROUP BY protocol_type, state\n                           ORDER BY 3 DESC\n                           \"\"\")\ndisplay(attack_protocol)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8014947-0f0f-4fcd-ad93-6ef23ad0544f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Well, looks like ICMP connections followed by TCP connections have had the maximum attacks!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c0edf61-bc5a-483d-9ced-d49f6384fa7f"}}},{"cell_type":"markdown","source":["### Connection stats based on protocols and attacks\n\nLet's take a look at some statistical measures pertaining to these protocols and attacks for our connection requests."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c187d38c-6748-47a8-b5cb-a6c75cd13380"}}},{"cell_type":"code","source":["attack_stats = spark.sql(\"\"\"\n                           SELECT \n                             protocol_type, \n                             CASE label\n                               WHEN 'normal.' THEN 'no attack'\n                               ELSE 'attack'\n                             END AS state,\n                             COUNT(*) as total_freq,\n                             ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n                             ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n                             ROUND(AVG(duration), 2) as mean_duration,\n                             SUM(num_failed_logins) as total_failed_logins,\n                             SUM(num_compromised) as total_compromised,\n                             SUM(num_file_creations) as total_file_creations,\n                             SUM(su_attempted) as total_root_attempts,\n                             SUM(num_root) as total_root_acceses\n                           FROM connections\n                           GROUP BY protocol_type, state\n                           ORDER BY 3 DESC\n                           \"\"\")\ndisplay(attack_stats)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cea38788-7597-40a9-bb98-95d0b15e7366"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Looks like average amount of data being transmitted in TCP requests are much higher which is not surprising. Interestingly attacks have a much higher average payload of data being transmitted from the source to the destination."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"170d94a1-f60c-46a4-9303-73df09274339"}}},{"cell_type":"markdown","source":["#### Filtering connection stats based on the TCP protocol by service and attack type\n\nLet's take a closer look at TCP attacks given that we have more relevant data and statistics for the same. We will now aggregate different types of TCP attacks based on service, attack type and observe different metrics."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10a1eba5-8573-4f25-9b07-aab3fb9fc8fd"}}},{"cell_type":"code","source":["tcp_attack_stats = spark.sql(\"\"\"\n                                   SELECT \n                                     service,\n                                     label as attack_type,\n                                     COUNT(*) as total_freq,\n                                     ROUND(AVG(duration), 2) as mean_duration,\n                                     SUM(num_failed_logins) as total_failed_logins,\n                                     SUM(num_file_creations) as total_file_creations,\n                                     SUM(su_attempted) as total_root_attempts,\n                                     SUM(num_root) as total_root_acceses\n                                   FROM connections\n                                   WHERE protocol_type = 'tcp'\n                                   AND label != 'normal.'\n                                   GROUP BY service, attack_type\n                                   ORDER BY total_freq DESC\n                                   \"\"\")\ndisplay(tcp_attack_stats)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"595b9df8-d698-4428-91fd-95800d0a5b97"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["There are a lot of attack types and the preceding output shows a specific section of the same."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de2ee654-3767-474f-9561-1cb6bad4e3b2"}}},{"cell_type":"markdown","source":["### Filtering connection stats based on the TCP protocol by service and attack type\n\nWe will now filter some of these attack types by imposing some constraints based on duration, file creations, root accesses in our query."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"214ed26c-01c3-410f-ab9b-959573d4d30b"}}},{"cell_type":"code","source":["tcp_attack_stats = spark.sql(\"\"\"\n                                   SELECT \n                                     service,\n                                     label as attack_type,\n                                     COUNT(*) as total_freq,\n                                     ROUND(AVG(duration), 2) as mean_duration,\n                                     SUM(num_failed_logins) as total_failed_logins,\n                                     SUM(num_file_creations) as total_file_creations,\n                                     SUM(su_attempted) as total_root_attempts,\n                                     SUM(num_root) as total_root_acceses\n                                   FROM connections\n                                   WHERE (protocol_type = 'tcp'\n                                          AND label != 'normal.')\n                                   GROUP BY service, attack_type\n                                   HAVING (mean_duration >= 50\n                                           AND total_file_creations >= 5\n                                           AND total_root_acceses >= 1)\n                                   ORDER BY total_freq DESC\n                                   \"\"\")\ndisplay(tcp_attack_stats)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f7be946-0e24-4a06-9830-75b4eb17e602"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Interesting to see multihop attacks being able to get root accesses to the destination hosts!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ac8cd2e-3d79-4676-97e2-dc2e91a7c2d2"}}},{"cell_type":"markdown","source":["### Subqueries to filter TCP attack types based on service\n\nLet's try to get all the TCP attacks based on service and attack type such that the overall mean duration of these attacks is greater than zero (`> 0`). For this we can do an inner query with all aggregation statistics and then extract the relevant queries and apply a mean duration filter in the outer query as shown below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c0ad1d3-eb45-44f4-a4d4-db5a183324f1"}}},{"cell_type":"code","source":["tcp_attack_stats = spark.sql(\"\"\"\n                                   SELECT \n                                     t.service,\n                                     t.attack_type,\n                                     t.total_freq\n                                   FROM\n                                   (SELECT \n                                     service,\n                                     label as attack_type,\n                                     COUNT(*) as total_freq,\n                                     ROUND(AVG(duration), 2) as mean_duration,\n                                     SUM(num_failed_logins) as total_failed_logins,\n                                     SUM(num_file_creations) as total_file_creations,\n                                     SUM(su_attempted) as total_root_attempts,\n                                     SUM(num_root) as total_root_acceses\n                                   FROM connections\n                                   WHERE protocol_type = 'tcp'\n                                   AND label != 'normal.'\n                                   GROUP BY service, attack_type\n                                   ORDER BY total_freq DESC) as t\n                                     WHERE t.mean_duration > 0 \n                                   \"\"\")\ndisplay(tcp_attack_stats)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13751759-1331-49d3-a211-6b3b5dea8a22"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This is nice! Now an interesting way to also view this data is to use a pivot table where one attribute represents rows and another one represents columns. Let's see if we can leverage Spark DataFrames to do this!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07cdf2cf-a534-4f61-88e4-b31571ecfc3f"}}},{"cell_type":"markdown","source":["### Building a Pivot Table from Aggregated Data\n\nHere, we will build upon the previous DataFrame object we obtained where we aggregated attacks based on type and service. For this, we can leverage the power of Spark DataFrames and the DataFrame DSL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2da9d8c8-5e7c-43d2-94c0-65a06f0ef341"}}},{"cell_type":"code","source":["display((tcp_attack_stats.groupby('service')\n                         .pivot('attack_type')\n                         .agg({'total_freq':'max'})\n                         .na.fill(0))\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ece2083-8aac-43fa-ad08-cc9614ba190b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We get a nice neat pivot table showing all the occurences based on service and attack type!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff19b618-25f4-437a-a0a4-2f85ed57f8bb"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Working with SQL at Scale - Spark SQL Tutorial","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2020994323503475}},"nbformat":4,"nbformat_minor":0}
